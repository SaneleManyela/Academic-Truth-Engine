{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f115a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "üìÇ 'Academic_Truth_Engine.ipynb' has been created! You can now download it from your file menu.\n",
      "üìÇ 'Academic_Truth_Engine.ipynb' has been created! You can now download it from your file menu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q nbformat\n",
    "import nbformat as nbf\n",
    "\n",
    "print(\"üìÇ 'Academic_Truth_Engine.ipynb' has been created! You can now download it from your file menu.\")\n",
    "\n",
    "# Create the notebook object\n",
    "nb = nbf.v4.new_notebook()\n",
    "cells = []\n",
    "\n",
    "# --- Step 0: The PowerShell Foundation ---\n",
    "cells.append(nbf.v4.new_markdown_cell(\n",
    "    \"# üìì The Academic Truth Engine\\n\"\n",
    "    \"**Goal:** 80%+ Accuracy in Leadership & Statistics\\n\"\n",
    "    \"**System:** IBM Granite 3.0 via Replicate\\n\\n\"\n",
    "    \"### Step 0: Initializing the Environment\\n\"\n",
    "    \"This project was initialized via **Windows PowerShell** to ensure local kernel stability.\\n\"\n",
    "    \"- **Command:** `jupyter notebook`\"\n",
    "))\n",
    "\n",
    "# --- Step 1: Tool Installation ---\n",
    "cells.append(nbf.v4.new_code_cell(\n",
    "    \"# Step 1: Install Academic Research Tools\\n\"\n",
    "    \"%pip install -q llama-index llama-index-llms-replicate llama-index-embeddings-huggingface \\\\\\n\"\n",
    "    \"  llama-index-readers-file llama-index-packs-fusion-retriever sentence-transformers \\\\\\n\"\n",
    "    \"  nest-asyncio requests replicate\\n\\n\"\n",
    "    \"import nest_asyncio\\n\"\n",
    "    \"nest_asyncio.apply()\\n\"\n",
    "    \"print('‚úÖ Tools installed successfully')\"\n",
    "))\n",
    "\n",
    "# --- Step 2: Granite 3.0 Logic ---\n",
    "cells.append(nbf.v4.new_code_cell(\n",
    "    \"# Step 2: Configure IBM Granite & Guardrails\\n\"\n",
    "    \"import os\\n\"\n",
    "    \"from getpass import getpass\\n\"\n",
    "    \"from llama_index.core import Settings\\n\"\n",
    "    \"from llama_index.llms.replicate import Replicate\\n\"\n",
    "    \"from llama_index.embeddings.huggingface import HuggingFaceEmbedding\\n\\n\"\n",
    "    \"# Replicate API Key handles the connection to Granite 3.0\\n\"\n",
    "    \"os.environ['REPLICATE_API_TOKEN'] = getpass('Enter your REPLICATE_API_KEY: ')\\n\\n\"\n",
    "    \"llm = Replicate(\\n\"\n",
    "    \"    model='ibm-granite/granite-3.0-8b-instruct',\\n\"\n",
    "    \"    temperature=0.1,  # Low temperature for high accuracy\\n\"\n",
    "    \"    system_prompt=(\\n\"\n",
    "    \"        'You are an expert academic RAG system. Answer ONLY using the provided context. '\\n\"\n",
    "    \"        'Never hallucinate. Never editorialize. If the answer is not in the context, say so. '\\n\"\n",
    "    \"        'Provide factual responses with 2-4 evidence bullets.'\\n\"\n",
    "    \"    )\\n\"\n",
    "    \")\\n\"\n",
    "    \"Settings.llm = llm\\n\"\n",
    "    \"Settings.embed_model = HuggingFaceEmbedding(model_name='BAAI/bge-small-en-v1.5')\\n\"\n",
    "    \"print('‚úÖ Granite 3.0 Guardrails Active')\"\n",
    "))\n",
    "\n",
    "# --- Step 3: Google Drive Ingestion ---\n",
    "cells.append(nbf.v4.new_code_cell(\n",
    "    \"# Step 3: PDF Ingestion from Google Drive\\n\"\n",
    "    \"import requests, re\\n\\n\"\n",
    "    \"def download_pdf(url, path):\\n\"\n",
    "    \"    match = re.search(r'/d/([A-Za-z0-9_-]+)', url)\\n\"\n",
    "    \"    f_id = match.group(1) if match else re.search(r'id=([A-Za-z0-9_-]+)', url).group(1)\\n\"\n",
    "    \"    d_url = f'https://drive.google.com/uc?export=download&id={f_id}'\\n\"\n",
    "    \"    r = requests.get(d_url)\\n\"\n",
    "    \"    with open(path, 'wb') as f: f.write(r.content)\\n\"\n",
    "    \"    print(f'‚úÖ PDF Secured: {path}')\\n\\n\"\n",
    "    \"drive_link = input('üìå Paste Google Drive Link: ').strip()\\n\"\n",
    "    \"os.makedirs('data', exist_ok=True)\\n\"\n",
    "    \"pdf_path = 'data/source.pdf'\\n\"\n",
    "    \"download_pdf(drive_link, pdf_path)\"\n",
    "))\n",
    "\n",
    "# --- Step 4: Semantic Chunking ---\n",
    "cells.append(nbf.v4.new_markdown_cell(\n",
    "    \"### Step 4: Semantic Chunking\\n\"\n",
    "    \"We use semantic splitting to ensure that leadership theories and statistical data stay intact.\"\n",
    "))\n",
    "cells.append(nbf.v4.new_code_cell(\n",
    "    \"from llama_index.core import SimpleDirectoryReader\\n\"\n",
    "    \"from llama_index.core.node_parser import SemanticSplitterNodeParser\\n\\n\"\n",
    "    \"docs = SimpleDirectoryReader(input_files=[pdf_path]).load_data()\\n\"\n",
    "    \"parser = SemanticSplitterNodeParser(buffer_size=3, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model)\\n\"\n",
    "    \"nodes = parser.get_nodes_from_documents(docs)\\n\"\n",
    "    \"for n in nodes: n.metadata['source'] = 'source.pdf'\\n\"\n",
    "    \"print(f'üîç Created {len(nodes)} high-fidelity semantic nodes.')\"\n",
    "))\n",
    "\n",
    "# --- Step 5: Query Fusion ---\n",
    "cells.append(nbf.v4.new_markdown_cell(\n",
    "    \"### Step 5: Query Fusion\\n\"\n",
    "    \"This engine rewrites your question 6 times to ensure no fact is missed.\"\n",
    "))\n",
    "cells.append(nbf.v4.new_code_cell(\n",
    "    \"from llama_index.core.llama_pack import download_llama_pack\\n\"\n",
    "    \"QueryRewritingRetrieverPack = download_llama_pack('QueryRewritingRetrieverPack', './query_rewriting_pack')\\n\"\n",
    "    \"query_rewriting_pack = QueryRewritingRetrieverPack(nodes, chunk_size=256, vector_similarity_top_k=5, num_queries=6)\\n\"\n",
    "    \"print('üöÄ Query Fusion Engine Live')\"\n",
    "))\n",
    "\n",
    "# --- Step 6: Interactive Loop ---\n",
    "cells.append(nbf.v4.new_code_cell(\n",
    "    \"# Step 6: Final Academic Research Loop\\n\"\n",
    "    \"print('--- RESEARCH MODE ACTIVE (Type \\\"end\\\" to exit) ---')\\n\"\n",
    "    \"while True:\\n\"\n",
    "    \"    q = input('\\\\nüü¶ Research Question: ').strip()\\n\"\n",
    "    \"    if q.lower() == 'end': break\\n\"\n",
    "    \"    ans = query_rewriting_pack.run(q)\\n\"\n",
    "    \"    print(f'\\\\nüß† FACTUAL ANALYSIS:\\\\n{ans}')\\n\"\n",
    "    \"    print('\\\\nüìç SOURCE: Absolute Reference to source.pdf')\"\n",
    "))\n",
    "\n",
    "# Write to file\n",
    "# ...existing code...\n",
    "nb['cells'] = cells\n",
    "with open('Academic_Truth_Engine.ipynb', 'w', encoding='utf-8') as f:\n",
    "    nbf.write(nb, f)\n",
    "# ...existing code...\n",
    "print(\"üìÇ 'Academic_Truth_Engine.ipynb' has been created! You can now download it from your file menu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1abb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
